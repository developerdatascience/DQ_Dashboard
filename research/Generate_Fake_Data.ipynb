{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "837fac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"faker\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94bf5ba0-ab7a-4485-b919-be9655222cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 20250222.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize Faker for generating fake data\n",
    "fake = Faker()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of rows\n",
    "num_rows = 14300\n",
    "\n",
    "# Generate synthetic credit card fraud dataset\n",
    "data = {\n",
    "    \"TransactionID\": np.arange(1, num_rows + 1),\n",
    "    \"UserID\": np.random.randint(1000, 5000, size=num_rows),\n",
    "    \"TransactionAmount\": np.round(np.random.uniform(5, 5000, size=num_rows), 2),\n",
    "    \"TransactionDate\": pd.date_range(start=\"2024-01-01\", periods=num_rows, freq=\"T\"),\n",
    "    \"TransactionType\": np.random.choice([\"Online\", \"POS\", \"ATM Withdrawal\"], size=num_rows),\n",
    "    \"Merchant\": [fake.company() for _ in range(num_rows)],\n",
    "    \"Location\": [fake.city() for _ in range(num_rows)],\n",
    "    \"CardType\": np.random.choice([\"Visa\", \"MasterCard\", \"Amex\", \"Discover\"], size=num_rows),\n",
    "    \"IsFraud\": np.random.choice([0, 1], size=num_rows, p=[0.98, 0.02]),  # 2% fraud cases\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce missing values randomly\n",
    "for col in [\"TransactionAmount\", \"TransactionType\", \"Merchant\", \"TransactionDate\", \"CardType\"]:\n",
    "    df.loc[df.sample(frac=0.03).index, col] = np.nan\n",
    "\n",
    "# Introduce missing values randomly\n",
    "for col in [ \"Merchant\", \"TransactionDate\", \"CardType\"]:\n",
    "    df.loc[df.sample(frac=0.02).index, col] = np.nan\n",
    "\n",
    "# Introduce duplicate rows\n",
    "df = pd.concat([df, df.sample(frac=0.04)], ignore_index=True)\n",
    "\n",
    "# Introduce inconsistent formats in the \"Location\" column\n",
    "df.loc[df.sample(frac=0.01).index, \"Location\"] = df[\"Location\"].apply(lambda x: x.lower())\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "filename = f\"{current_date}.csv\"\n",
    "print(f\"file: {filename}\")\n",
    "\n",
    "# Save to CSV\n",
    "file_path = f\"../inputs/finance/{filename}\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_records = {}\n",
    "for file in glob.glob(pathname=\"../archive/*.csv\"):\n",
    "    filename = file.split(\"/\")[2].split(\".\")[0]\n",
    "    count = spark.read.csv(file).count()\n",
    "    file_records[filename] = count\n",
    "\n",
    "file_records\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "yesterday = yesterday.strftime(\"%Y%m%d\")\n",
    "print(yesterday)\n",
    "\n",
    "if Path(f\"../inputs/finance/{yesterday}.csv\").exists():\n",
    "    print(f\"{yesterday} exists\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(f\"../inputs/finance/{current_date}\")\n",
    "\n",
    "if not folder_path.exists():\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Folder {folder_path} created\")\n",
    "else:\n",
    "    print(f\"Folder {folder_path} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa998b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = glob.glob(\"../inputs/finance/*\")\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "for file in list_of_files:\n",
    "    if file != latest_file:\n",
    "        shutil.move(src=file, dst=\"archive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b297b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_old_file_to_archive():\n",
    "    archive_path = Path(f\"../archive/\")\n",
    "    input_container = Path(\"../inputs/finance\")\n",
    "\n",
    "    if not archive_path.exists():\n",
    "        archive_path.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"folder {archive_path} exists\")\n",
    "    \n",
    "    list_of_files = glob.glob(f\"{input_container}/*\")\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "    for file in list_of_files:\n",
    "        if file != latest_file:\n",
    "            shutil.move(src=file, dst=archive_path)\n",
    "            print(f\"{file} moved successfully to {archive_path}\")\n",
    "        else:\n",
    "            print(\"Path does not exists\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_old_file_to_archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d351255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_records():\n",
    "    file_records = {}\n",
    "    for file in glob.glob(pathname=\"../archive/*.*\"):\n",
    "        filename = file.split(\"/\")[2].split(\".\")[0]\n",
    "        print(filename)\n",
    "        count = spark.read.csv(file).count()\n",
    "        file_records[filename] = count\n",
    "    \n",
    "    schema = StructType(fields=\n",
    "                            [\n",
    "                            StructField(\"Date\", StringType(), False),\n",
    "                            StructField(\"Value\", IntegerType(), False)\n",
    "                            ]\n",
    "                            )\n",
    "\n",
    "    df = spark.createDataFrame([(k, v) for k, v in file_records.items()], schema)\n",
    "    df = df.withColumn(\"Date\", to_date(col(\"Date\"), format=\"yyyyMMdd\"))\n",
    "    df = df.orderBy(col(\"Date\").desc())\n",
    "    pd_df = df.toPandas()\n",
    "    # pd_df.to_csv(\"../artifacts/data_records.csv\")\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5fd492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250219\n",
      "20250218\n",
      "20250220\n",
      "20250221\n",
      "20250216\n",
      "20250217\n",
      "20250215\n",
      "20250214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>29641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>20401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-19</td>\n",
       "      <td>15601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>18721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-17</td>\n",
       "      <td>19761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Value\n",
       "0  2025-02-21  29641\n",
       "1  2025-02-20  20401\n",
       "2  2025-02-19  15601\n",
       "3  2025-02-18  18721\n",
       "4  2025-02-17  19761"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = get_past_records()\n",
    "# new_df[\"Date\"] = pd.to_datetime(new_df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d21c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='archive', Value=5201)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1893df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for archive/20250220 path exist\n",
    "archive_path = Path(f\"../archive/{yesterday}/\")\n",
    "\n",
    "old_file_path = Path(f\"../inputs/finance/{yesterday}.csv\")\n",
    "\n",
    "if not archive_path.exists():\n",
    "    archive_path.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    print(f\"folder {archive_path} exists\")\n",
    "\n",
    "if old_file_path.exists():\n",
    "    shutil.move(src=old_file_path, dst=archive_path)\n",
    "    print(\"file moved successfully\")\n",
    "else:\n",
    "    print(\"Path does not exists\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56da786",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(f\"../inputs/finance/{yesterday}.csv\").exists():\n",
    "    print(f\"{yesterday} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62f0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../inputs/finance/20250221.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b83985",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = Path(f\"../inputs/finance/{yesterday}.csv\")\n",
    "destination_path = archive_path / f\"{yesterday}.csv\"\n",
    "\n",
    "if source_path.exists():\n",
    "    source_path.rename(destination_path)\n",
    "    print(f\"File moved to {destination_path}\")\n",
    "else:\n",
    "    print(f\"Source file {source_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8855a-126f-46d9-922e-dffc31d65f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key=\"\",\n",
    "aws_secret_access_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=\"\",\n",
    "    aws_secret_access_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1745aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.put_object(Bucket=\"creditcardfraudcontainer\", Key=\"bronze/credit_card_fraud.csv\", Body=\"../inputs/finance/credit_card_fraud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb043fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in s3.list_buckets()[\"Buckets\"]:\n",
    "    print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket=\"creditcardfraudcontainer\")\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eaba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ffb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Logger setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AWS_S3:\n",
    "    def __init__(self, bucketname: str, access_key: str, secret_key: str) -> None:\n",
    "        self.bucketname = bucketname\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "        if not self.access_key or not self.secret_key:\n",
    "            logger.error(\"Please provide access and secret keys to continue\")\n",
    "            raise ValueError(\"Missing AWS credentials\")\n",
    "\n",
    "        if not self.bucketname:\n",
    "            logger.error(\"Please provide a valid bucket name\")\n",
    "            raise ValueError(\"Bucket name is required\")\n",
    "\n",
    "    def _connect_to_aws(self):\n",
    "        \"\"\"Create and return an S3 client\"\"\"\n",
    "        try:\n",
    "            s3 = boto3.client(\n",
    "                \"s3\",\n",
    "                aws_access_key_id=self.access_key,\n",
    "                aws_secret_access_key=self.secret_key\n",
    "            )\n",
    "            logger.info(\"Successfully connected to AWS S3\")\n",
    "            return s3  # ✅ RETURN the S3 client\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection failed: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def upload_file_to_s3(self, s3_key: str, local_file_path: str) -> None:\n",
    "        \"\"\"Upload a file to S3\"\"\"\n",
    "        s3 = self._connect_to_aws()  # ✅ Fix: Get S3 client\n",
    "        \n",
    "        try:\n",
    "            with open(local_file_path, \"rb\") as file:\n",
    "                s3.put_object(Bucket=self.bucketname, Key=s3_key, Body=file)\n",
    "            logger.info(f\"File {local_file_path} uploaded successfully to S3 as {s3_key}\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"File Upload Failed. Check credentials and permissions\")\n",
    "            raise e\n",
    "    \n",
    "    def read_small_data_from_s3(self, s3_key: str):\n",
    "        \"\"\"Read a small CSV file from S3\"\"\"\n",
    "        s3 = self._connect_to_aws()  # ✅ Fix: Get S3 client\n",
    "        \n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=self.bucketname, Key=s3_key)  # ✅ Fix: Use `Key=`\n",
    "            df = pd.read_csv(StringIO(obj[\"Body\"].read().decode(\"utf-8\")))\n",
    "            logger.info(f\"Successfully read data from S3: {s3_key}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred while reading data from AWS: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = AWS_S3(\n",
    "    bucketname=\"creditcardfraudcontainer\",\n",
    "    access_key=aws_access_key,\n",
    "    secret_key=aws_secret_access_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54249550",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.upload_file_to_s3(\n",
    "    s3_key=\"bronze/credit_card_fraud.csv\", \n",
    "    local_file_path='../inputs/finance/credit_card_fraud.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "local_path = os.path.abspath(\"../inputs/finance/credit_card_fraud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b581ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL-6mpoCg48",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
